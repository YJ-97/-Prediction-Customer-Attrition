{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from imblearn.under_sampling import OneSidedSelection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../datasets/BankChurners.csv')\n",
    "list = ['Attrition_Flag', 'Total_Trans_Ct', 'Total_Trans_Amt', 'Total_Revolving_Bal', 'Total_Ct_Chng_Q4_Q1', 'Contacts_Count_12_mon', 'Total_Relationship_Count', 'Months_Inactive_12_mon', 'Months_on_book']\n",
    "data = data[list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category : [0 1]\n",
      "classes : ['Attrited Customer' 'Existing Customer']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "object_columns = data.select_dtypes('object').columns\n",
    "\n",
    "for i in object_columns:\n",
    "\n",
    "    lb = LabelEncoder()\n",
    "    lb.fit(data[i])\n",
    "    data[i] = lb.transform(data[i])\n",
    "    \n",
    "    print(f'category : {np.unique(data[i])}\\nclasses : {lb.classes_}\\n')\n",
    "\n",
    "input = data.iloc[:,1:]\n",
    "target = data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBClassifier\n",
    "##### 설명 참고 : https://wooono.tistory.com/97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best f1 : 0.9790588684973764\n",
      "best param :  {'gamma': 1, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_test = ss.fit_transform(x_test)\n",
    "\n",
    "oss = OneSidedSelection()\n",
    "x_train, y_train = oss.fit_resample(x_train, y_train)\n",
    "\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "\n",
    "xgb_param_grid = {'n_estimators' : [100, 200],\n",
    "                'learning_rate' : [0.01, 0.05, 0.1],\n",
    "                'max_depth' : [3, 5, 7],\n",
    "                'gamma' : [0, 1, 2]}\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb, param_grid=xgb_param_grid, scoring='f1', verbose=0, n_jobs=1)\n",
    "xgb_grid.fit(x_train, y_train)\n",
    "\n",
    "print(f'best f1 : {xgb_grid.best_score_}')\n",
    "print('best param : ', xgb_grid.best_params_)\n",
    "\n",
    "## 참고 : https://cjh34544.tistory.com/m/4\n",
    "## http://aispiration.com/model/model-python-xgboost-hyper.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best f1 : 0.9432743954066511\n",
      "best param :  {'C': 0.1, 'penalty': 'l2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "25 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"d:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "d:\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.92285593        nan 0.94138575        nan 0.9432744\n",
      "        nan 0.94289834        nan 0.94275006]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_test = ss.fit_transform(x_test)\n",
    "\n",
    "oss = OneSidedSelection()\n",
    "x_train, y_train = oss.fit_resample(x_train, y_train)\n",
    "\n",
    "lr = LogisticRegression(random_state=42)\n",
    "\n",
    "lr_param_grid = {'C' : [0.001, 0.01, 0.1, 1, 10],\n",
    "                'penalty' : ['l1', 'l2']}\n",
    "\n",
    "lr_grid = GridSearchCV(lr, param_grid=lr_param_grid, scoring='f1', verbose=0, n_jobs=1)\n",
    "lr_grid.fit(x_train, y_train)\n",
    "\n",
    "print(f'best f1 : {lr_grid.best_score_}')\n",
    "print('best param : ', lr_grid.best_params_)\n",
    "\n",
    "# 참고 : https://wikidocs.net/16594\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best f1 : 0.967217153494609\n",
      "best param :  {'max_depth': 7, 'min_samples_leaf': 8, 'min_samples_split': 20, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_test = ss.fit_transform(x_test)\n",
    "\n",
    "oss = OneSidedSelection()\n",
    "x_train, y_train = oss.fit_resample(x_train, y_train)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rf_param_grid = {'n_estimators' : [100, 200],\n",
    "                'max_depth' : [3, 5, 7],\n",
    "                'min_samples_leaf' : [8, 12, 16],\n",
    "                'min_samples_split' : [8, 16, 20]}\n",
    "\n",
    "rf_grid = GridSearchCV(rf, param_grid=rf_param_grid, scoring='f1', verbose=0, n_jobs=1)\n",
    "rf_grid.fit(x_train, y_train)\n",
    "\n",
    "print(f'best f1 : {rf_grid.best_score_}')\n",
    "print('best param : ', rf_grid.best_params_)\n",
    "\n",
    "## 참고 : https://techblog-history-younghunjo1.tistory.com/102\n",
    "## https://jaaamj.tistory.com/35\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best f1 : 0.9776136754152412\n",
      "best param :  {'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_test = ss.fit_transform(x_test)\n",
    "\n",
    "oss = OneSidedSelection(random_state=42)\n",
    "x_train, y_train = oss.fit_resample(x_train, y_train)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rf_param_grid = {'n_estimators' : [100],\n",
    "                'min_samples_leaf' : [1],\n",
    "                'min_samples_split' : [2]}\n",
    "\n",
    "rf_grid = GridSearchCV(rf, param_grid=rf_param_grid, scoring='f1', verbose=0, n_jobs=1)\n",
    "rf_grid.fit(x_train, y_train)\n",
    "\n",
    "print(f'best f1 : {rf_grid.best_score_}')\n",
    "print('best param : ', rf_grid.best_params_)\n",
    "\n",
    "## 참고 : https://techblog-history-younghunjo1.tistory.com/102\n",
    "## https://jaaamj.tistory.com/35\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CatBoostClassifier는 파라미터 조정이 성능에 크게 영향을 미치지 않는다는 말이 많아 일단 생략함\n",
    "##### https://velog.io/@jus6886/Catboost\n",
    "##### https://undeadkwandoll.tistory.com/61\n",
    "##### https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002698429\n",
    "#### CatBoost 설명\n",
    "##### https://dailyheumsi.tistory.com/136\n",
    "##### https://techblog-history-younghunjo1.tistory.com/199\n",
    "##### https://heeya-stupidbutstudying.tistory.com/43?category=950711\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lightGBM은 10000개 이하의 데이터에 overfitting하기 쉬워서 사용 x\n",
    "##### https://nurilee.com/2020/04/03/lightgbm-definition-parameter-tuning/\n",
    "##### https://mac-user-guide.tistory.com/79"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting\n",
    "##### 코드 참고 : https://eunki.tistory.com/60\n",
    "##### https://nonmeyet.tistory.com/entry/Python-Voting-Classifiers%EB%8B%A4%EC%88%98%EA%B2%B0-%EB%B6%84%EB%A5%98%EC%9D%98-%EC%A0%95%EC%9D%98%EC%99%80-%EA%B5%AC%ED%98%84\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 모델 4개 사용한 hard voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9639684106614018 recall : 0.9864626250735727, precision : 0.9710312862108922, f1 : 0.9786861313868612\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_test = ss.fit_transform(x_test)\n",
    "\n",
    "oss = OneSidedSelection(random_state=42)\n",
    "x_train, y_train = oss.fit_resample(x_train, y_train)\n",
    "\n",
    "xgb = XGBClassifier(gamma=1, learning_rate=0.1, max_depth=5, n_estimators=200)\n",
    "lr = LogisticRegression(C=0.1, penalty='l2')\n",
    "rf = RandomForestClassifier(verbose=0)\n",
    "cat = CatBoostClassifier(verbose=0)\n",
    "\n",
    "model = VotingClassifier(estimators=[('xgb', xgb), ('lr', lr), ('rf', rf), ('cat', cat)], weights=[2, 1, 2, 2], voting='hard')\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "acc = accuracy_score(y_test, pred)\n",
    "recall = recall_score(y_test, pred)\n",
    "precision = precision_score(y_test, pred)\n",
    "f1 = f1_score(y_test, pred)\n",
    "\n",
    "print('accuracy : {0} recall : {1}, precision : {2}, f1 : {3}'.format(acc, recall, precision, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 4개 사용한 soft voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9615004935834156 recall : 0.987051206592113, precision : 0.9676860934795153, f1 : 0.9772727272727273\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_test = ss.fit_transform(x_test)\n",
    "\n",
    "oss = OneSidedSelection(random_state=42)\n",
    "x_train, y_train = oss.fit_resample(x_train, y_train)\n",
    "\n",
    "xgb = XGBClassifier(gamma=1, learning_rate=0.1, max_depth=5, n_estimators=200)\n",
    "lr = LogisticRegression(C=0.1, penalty='l2')\n",
    "rf = RandomForestClassifier(verbose=0)\n",
    "cat = CatBoostClassifier(verbose=0)\n",
    "\n",
    "model = VotingClassifier(estimators=[('xgb', xgb), ('lr', lr), ('rf', rf), ('cat', cat)], weights=[2, 1, 2, 2], voting='soft')\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "acc = accuracy_score(y_test, pred)\n",
    "recall = recall_score(y_test, pred)\n",
    "precision = precision_score(y_test, pred)\n",
    "f1 = f1_score(y_test, pred)\n",
    "\n",
    "print('accuracy : {0} recall : {1}, precision : {2}, f1 : {3}'.format(acc, recall, precision, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 3개 사용한 hard voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9644619940769991 recall : 0.9864626250735727, precision : 0.9715942028985507, f1 : 0.9789719626168224\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_test = ss.fit_transform(x_test)\n",
    "\n",
    "oss = OneSidedSelection(random_state=42)\n",
    "x_train, y_train = oss.fit_resample(x_train, y_train)\n",
    "\n",
    "xgb = XGBClassifier(gamma=1, learning_rate=0.1, max_depth=5, n_estimators=200)\n",
    "lr = LogisticRegression(C=0.1, penalty='l2')\n",
    "rf = RandomForestClassifier(verbose=0)\n",
    "cat = CatBoostClassifier(verbose=0)\n",
    "\n",
    "model = VotingClassifier(estimators=[('xgb', xgb), ('rf', rf), ('cat', cat)], weights=[1, 1, 1], voting='hard')\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "acc = accuracy_score(y_test, pred)\n",
    "recall = recall_score(y_test, pred)\n",
    "precision = precision_score(y_test, pred)\n",
    "f1 = f1_score(y_test, pred)\n",
    "roc = roc_auc_score(y_test, pred)\n",
    "\n",
    "print('accuracy : {0} recall : {1}, precision : {2}, f1 : {3}'.format(acc, recall, precision, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 3개 사용한 soft voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9649555774925962 recall : 0.985285462036492, precision : 0.9732558139534884, f1 : 0.9792336940625913\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_test = ss.fit_transform(x_test)\n",
    "\n",
    "oss = OneSidedSelection(random_state=42)\n",
    "x_train, y_train = oss.fit_resample(x_train, y_train)\n",
    "\n",
    "xgb = XGBClassifier(gamma=1, learning_rate=0.1, max_depth=5, n_estimators=200)\n",
    "lr = LogisticRegression(C=0.1, penalty='l2')\n",
    "rf = RandomForestClassifier(verbose=0)\n",
    "cat = CatBoostClassifier(verbose=0)\n",
    "\n",
    "model = VotingClassifier(estimators=[('xgb', xgb), ('rf', rf), ('cat', cat)], weights=[1, 1, 1], voting='soft')\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "acc = accuracy_score(y_test, pred)\n",
    "recall = recall_score(y_test, pred)\n",
    "precision = precision_score(y_test, pred)\n",
    "f1 = f1_score(y_test, pred)\n",
    "roc = roc_auc_score(y_test, pred)\n",
    "\n",
    "print('accuracy : {0} recall : {1}, precision : {2}, f1 : {3}'.format(acc, recall, precision, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 평가지표로 f1 score를 쓰는 이유\n",
    "##### https://towardsdatascience.com/read-this-before-using-roc-auc-as-a-metric-c84c2d5af621\n",
    "##### https://stackoverflow.com/questions/44172162/f1-score-vs-roc-auc\n",
    "##### https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting Model 비교\n",
    "##### https://medium.com/@divyagera2402/boosting-algorithms-adaboost-gradient-boosting-xgb-light-gbm-and-catboost-e7d2dbc4e4ca\n",
    "##### http://dmqm.korea.ac.kr/activity/seminar/323\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 인코딩 방법\n",
    "##### https://wyatt37.tistory.com/11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한 것요약\n",
    "##### 원핫인코딩은 차원을 늘려 과적합되기 쉽다. 물론 규제로 어느정도 커버할 수 있으나 트리계열에서는 해당 변수가 아예 제외되는 문제점을 가진다\n",
    "##### catboost는 파라미터 수정을 하지 않아도 효과가 나쁘지 않다\n",
    "##### lightgbm은 데이터의 수가 너무 적어 사용할 수 없다\n",
    "##### ROC가 F1보다 불균형 데이터 셋에 대해 관대한? 경향이 있어서 불균형 데이터 셋에는 F1을 평가지표로 사용한다.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1fa163922eb0b3709bbb5d8082b2465c9de796dbaacca80cbaa600e7fff3e4fe"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
