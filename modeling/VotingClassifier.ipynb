{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from imblearn.under_sampling import OneSidedSelection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../datasets/BankChurners.csv')\n",
    "list = ['Attrition_Flag', 'Total_Trans_Ct', 'Total_Trans_Amt', 'Total_Revolving_Bal', 'Total_Ct_Chng_Q4_Q1', 'Contacts_Count_12_mon', 'Total_Relationship_Count', 'Months_Inactive_12_mon', 'Months_on_book']\n",
    "data = data[list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category : [0 1]\n",
      "classes : ['Attrited Customer' 'Existing Customer']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "object_columns = data.select_dtypes('object').columns\n",
    "\n",
    "for i in object_columns:\n",
    "\n",
    "    lb = LabelEncoder()\n",
    "    lb.fit(data[i])\n",
    "    data[i] = lb.transform(data[i])\n",
    "    \n",
    "    print(f'category : {np.unique(data[i])}\\nclasses : {lb.classes_}\\n')\n",
    "\n",
    "input = data.iloc[:,1:]\n",
    "target = data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best f1 : 0.9790588684973764\n",
      "best param :  {'gamma': 1, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_test = ss.fit_transform(x_test)\n",
    "\n",
    "oss = OneSidedSelection()\n",
    "x_train, y_train = oss.fit_resample(x_train, y_train)\n",
    "\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "\n",
    "xgb_param_grid = {'n_estimators' : [100, 200],\n",
    "                'learning_rate' : [0.01, 0.05, 0.1],\n",
    "                'max_depth' : [3, 5, 7],\n",
    "                'gamma' : [0, 1, 2]}\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb, param_grid=xgb_param_grid, scoring='f1', verbose=0, n_jobs=1)\n",
    "xgb_grid.fit(x_train, y_train)\n",
    "\n",
    "print(f'best f1 : {xgb_grid.best_score_}')\n",
    "print('best param : ', xgb_grid.best_params_)\n",
    "\n",
    "## 참고 : https://cjh34544.tistory.com/m/4\n",
    "## http://aispiration.com/model/model-python-xgboost-hyper.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best f1 : 0.9432743954066511\n",
      "best param :  {'C': 0.1, 'penalty': 'l2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "25 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"d:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "d:\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.92285593        nan 0.94138575        nan 0.9432744\n",
      "        nan 0.94289834        nan 0.94275006]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_test = ss.fit_transform(x_test)\n",
    "\n",
    "oss = OneSidedSelection()\n",
    "x_train, y_train = oss.fit_resample(x_train, y_train)\n",
    "\n",
    "lr = LogisticRegression(random_state=42)\n",
    "\n",
    "lr_param_grid = {'C' : [0.001, 0.01, 0.1, 1, 10],\n",
    "                'penalty' : ['l1', 'l2']}\n",
    "\n",
    "lr_grid = GridSearchCV(lr, param_grid=lr_param_grid, scoring='f1', verbose=0, n_jobs=1)\n",
    "lr_grid.fit(x_train, y_train)\n",
    "\n",
    "print(f'best f1 : {lr_grid.best_score_}')\n",
    "print('best param : ', lr_grid.best_params_)\n",
    "\n",
    "# 참고 : https://wikidocs.net/16594\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best f1 : 0.967217153494609\n",
      "best param :  {'max_depth': 7, 'min_samples_leaf': 8, 'min_samples_split': 20, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_test = ss.fit_transform(x_test)\n",
    "\n",
    "oss = OneSidedSelection()\n",
    "x_train, y_train = oss.fit_resample(x_train, y_train)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rf_param_grid = {'n_estimators' : [100, 200],\n",
    "                'max_depth' : [3, 5, 7],\n",
    "                'min_samples_leaf' : [8, 12, 16],\n",
    "                'min_samples_split' : [8, 16, 20]}\n",
    "\n",
    "rf_grid = GridSearchCV(rf, param_grid=rf_param_grid, scoring='f1', verbose=0, n_jobs=1)\n",
    "rf_grid.fit(x_train, y_train)\n",
    "\n",
    "print(f'best f1 : {rf_grid.best_score_}')\n",
    "print('best param : ', rf_grid.best_params_)\n",
    "\n",
    "## 참고 : https://techblog-history-younghunjo1.tistory.com/102\n",
    "## https://jaaamj.tistory.com/35\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best f1 : 0.9776136754152412\n",
      "best param :  {'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_test = ss.fit_transform(x_test)\n",
    "\n",
    "oss = OneSidedSelection(random_state=42)\n",
    "x_train, y_train = oss.fit_resample(x_train, y_train)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rf_param_grid = {'n_estimators' : [100],\n",
    "                'min_samples_leaf' : [1],\n",
    "                'min_samples_split' : [2]}\n",
    "\n",
    "rf_grid = GridSearchCV(rf, param_grid=rf_param_grid, scoring='f1', verbose=0, n_jobs=1)\n",
    "rf_grid.fit(x_train, y_train)\n",
    "\n",
    "print(f'best f1 : {rf_grid.best_score_}')\n",
    "print('best param : ', rf_grid.best_params_)\n",
    "\n",
    "## 참고 : https://techblog-history-younghunjo1.tistory.com/102\n",
    "## https://jaaamj.tistory.com/35\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CatBoostClassifier는 파라미터 조정이 성능에 크게 영향을 미치지 않는다는 말이 많아 생략함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 모델 4개 사용한 hard voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9639684106614018 recall : 0.9864626250735727, precision : 0.9710312862108922, f1 : 0.9786861313868612\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_test = ss.fit_transform(x_test)\n",
    "\n",
    "oss = OneSidedSelection(random_state=42)\n",
    "x_train, y_train = oss.fit_resample(x_train, y_train)\n",
    "\n",
    "xgb = XGBClassifier(gamma=1, learning_rate=0.1, max_depth=5, n_estimators=200)\n",
    "lr = LogisticRegression(C=0.1, penalty='l2')\n",
    "rf = RandomForestClassifier(verbose=0)\n",
    "cat = CatBoostClassifier(verbose=0)\n",
    "\n",
    "model = VotingClassifier(estimators=[('xgb', xgb), ('lr', lr), ('rf', rf), ('cat', cat)], weights=[2, 1, 2, 2], voting='hard')\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "acc = accuracy_score(y_test, pred)\n",
    "recall = recall_score(y_test, pred)\n",
    "precision = precision_score(y_test, pred)\n",
    "f1 = f1_score(y_test, pred)\n",
    "\n",
    "print('accuracy : {0} recall : {1}, precision : {2}, f1 : {3}'.format(acc, recall, precision, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 4개 사용한 soft voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9615004935834156 recall : 0.987051206592113, precision : 0.9676860934795153, f1 : 0.9772727272727273\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_test = ss.fit_transform(x_test)\n",
    "\n",
    "oss = OneSidedSelection(random_state=42)\n",
    "x_train, y_train = oss.fit_resample(x_train, y_train)\n",
    "\n",
    "xgb = XGBClassifier(gamma=1, learning_rate=0.1, max_depth=5, n_estimators=200)\n",
    "lr = LogisticRegression(C=0.1, penalty='l2')\n",
    "rf = RandomForestClassifier(verbose=0)\n",
    "cat = CatBoostClassifier(verbose=0)\n",
    "\n",
    "model = VotingClassifier(estimators=[('xgb', xgb), ('lr', lr), ('rf', rf), ('cat', cat)], weights=[2, 1, 2, 2], voting='soft')\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "acc = accuracy_score(y_test, pred)\n",
    "recall = recall_score(y_test, pred)\n",
    "precision = precision_score(y_test, pred)\n",
    "f1 = f1_score(y_test, pred)\n",
    "\n",
    "print('accuracy : {0} recall : {1}, precision : {2}, f1 : {3}'.format(acc, recall, precision, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 3개 사용한 hard voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9644619940769991 recall : 0.9864626250735727, precision : 0.9715942028985507, f1 : 0.9789719626168224\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_test = ss.fit_transform(x_test)\n",
    "\n",
    "oss = OneSidedSelection(random_state=42)\n",
    "x_train, y_train = oss.fit_resample(x_train, y_train)\n",
    "\n",
    "xgb = XGBClassifier(gamma=1, learning_rate=0.1, max_depth=5, n_estimators=200)\n",
    "lr = LogisticRegression(C=0.1, penalty='l2')\n",
    "rf = RandomForestClassifier(verbose=0)\n",
    "cat = CatBoostClassifier(verbose=0)\n",
    "\n",
    "model = VotingClassifier(estimators=[('xgb', xgb), ('rf', rf), ('cat', cat)], weights=[1, 1, 1], voting='hard')\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "acc = accuracy_score(y_test, pred)\n",
    "recall = recall_score(y_test, pred)\n",
    "precision = precision_score(y_test, pred)\n",
    "f1 = f1_score(y_test, pred)\n",
    "roc = roc_auc_score(y_test, pred)\n",
    "\n",
    "print('accuracy : {0} recall : {1}, precision : {2}, f1 : {3}'.format(acc, recall, precision, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 3개 사용한 soft voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9649555774925962 recall : 0.985285462036492, precision : 0.9732558139534884, f1 : 0.9792336940625913\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(input, target, random_state=42, test_size=0.2)\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_test = ss.fit_transform(x_test)\n",
    "\n",
    "oss = OneSidedSelection(random_state=42)\n",
    "x_train, y_train = oss.fit_resample(x_train, y_train)\n",
    "\n",
    "xgb = XGBClassifier(gamma=1, learning_rate=0.1, max_depth=5, n_estimators=200)\n",
    "lr = LogisticRegression(C=0.1, penalty='l2')\n",
    "rf = RandomForestClassifier(verbose=0)\n",
    "cat = CatBoostClassifier(verbose=0)\n",
    "\n",
    "model = VotingClassifier(estimators=[('xgb', xgb), ('rf', rf), ('cat', cat)], weights=[1, 1, 1], voting='soft')\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "acc = accuracy_score(y_test, pred)\n",
    "recall = recall_score(y_test, pred)\n",
    "precision = precision_score(y_test, pred)\n",
    "f1 = f1_score(y_test, pred)\n",
    "roc = roc_auc_score(y_test, pred)\n",
    "\n",
    "print('accuracy : {0} recall : {1}, precision : {2}, f1 : {3}'.format(acc, recall, precision, f1))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1fa163922eb0b3709bbb5d8082b2465c9de796dbaacca80cbaa600e7fff3e4fe"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
